holds custom optimizers (lapropw, adahessian, etc).
for laprop gradient clipping likely is not needed.
will also test diffgrad soon.
